

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Linear Regression with Regularization &mdash; ML Tutorial  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="A Complete Guide to Matrix Notation and Linear Regression" href="linear_regression_tutorial.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> ML Tutorial
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="linear_regression_tutorial.html">A Complete Guide to Matrix Notation and Linear Regression</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Linear Regression with Regularization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#l2-penalty">L2 Penalty</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#closed-form-solution">Closed-form solution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#l1-penalty">L1 Penalty</a></li>
<li class="toctree-l2"><a class="reference internal" href="#l1-vs-l2-penalty">L1 vs L2 Penalty</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ML Tutorial</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>Linear Regression with Regularization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/blog_content/linear_regression/linear_regression_regularized_tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-blog-content-linear-regression-linear-regression-regularized-tutorial-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="linear-regression-with-regularization">
<span id="sphx-glr-blog-content-linear-regression-linear-regression-regularized-tutorial-py"></span><h1>Linear Regression with Regularization<a class="headerlink" href="#linear-regression-with-regularization" title="Permalink to this headline">¶</a></h1>
<p>Regularization is a way to prevent overfitting and allows the model to
generalize better.</p>
<p>Unlike polynomial fitting, it’s hard to imagine how linear regression
can overfit the data, since it’s just a single line (or a hyperplane).
One situation might be that features are <strong>correlated</strong> or redundant.
Suppose there are two features, both are exactly the same, our predicted
hyperplane will be in this format:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = w_0 + w_1x_1 + w_2x_2\]</div>
<p>and the true values of <span class="math notranslate nohighlight">\(x_2\)</span> is almost the same as <span class="math notranslate nohighlight">\(x_1\)</span> (or
with some multiplicative factor and noise). Then, it’s best to just drop
<span class="math notranslate nohighlight">\(w_2x_2\)</span> term and use:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = w_0 + w_1x_1\]</div>
<p>to fit the data. This is a simpler model.</p>
<p>But we don’t know whether <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> is <strong>actually</strong>
redundant or not, at least with bare eyes, and we don’t want to manually
drop a parameter just because we feel like it. We want to model to learn
to do this itself, that is, to <em>prefer a simpler model that fits the
data well enough</em>.</p>
<p>To do this, we add a penalty term to our loss function. Two common
penalty terms are L1 and L2.</p>
<div class="section" id="l2-penalty">
<h2>L2 Penalty<a class="headerlink" href="#l2-penalty" title="Permalink to this headline">¶</a></h2>
<p>Recall the loss function of linear regression from <a class="reference external" href="/blog_content/linear_regression/linear_regression_tutorial.html#writing-sse-loss-in-matrix-notation">previous
article</a>.</p>
<div class="math notranslate nohighlight">
\[L(w) = \sum_{i=1}^{n} \left( y^i - wx^i \right)^2\]</div>
<p>we can add the <strong>L2 penalty term</strong> to it, and this is called <strong>L2
regularization</strong>.:</p>
<div class="math notranslate nohighlight">
\[L(w) = \sum_{i=1}^{n} \left( y^i - wx^i \right)^2 + \lambda\sum_{j=0}^{d}w_j^2\]</div>
<p>This is called L2 penalty just because it’s a L2-norm of <span class="math notranslate nohighlight">\(w\)</span>. In
fancy term, this whole loss function is also known as <strong>Ridge
regression</strong>.</p>
<p>Let’s see what’s going on. Loss function is something we <strong>minimize</strong>.
Any terms that we add to it, we also want it to be minimized (that’s why
it’s called <em>penalty term</em>). The above means we want <span class="math notranslate nohighlight">\(w\)</span> that fits
the data well (first term), but we also want the values of <span class="math notranslate nohighlight">\(w\)</span> to
be small as possible (second term). <span class="math notranslate nohighlight">\(\lambda\)</span> (or whatever greek
signs used) is to adjust how much to penalize <span class="math notranslate nohighlight">\(w\)</span>. It’s impossible
to know the appropriate value for lambda. You just have to try them out,
in exponential range (0.01, 0.1, 1, 10, etc), then select the one that
has the lowest loss on validation set, or doing k-fold cross validation.</p>
<p>Setting <span class="math notranslate nohighlight">\(\lambda\)</span> to be very low means we don’t penalize the
complex model much. Setting it to <span class="math notranslate nohighlight">\(0\)</span> is the vanilla linear
regression. Setting it high means we strongly prefer simpler model, at
the cost of how well it fits the data.</p>
<div class="section" id="closed-form-solution">
<h3>Closed-form solution<a class="headerlink" href="#closed-form-solution" title="Permalink to this headline">¶</a></h3>
<p>We can also write L2 regression in a nice matrix notation:</p>
<div class="math notranslate nohighlight">
\[L(w) = {\left\lVert y - Xw \right\rVert}^2 + \lambda{\left\lVert w \right\rVert}_2^2\]</div>
<p>Then the gradient is:</p>
<div class="math notranslate nohighlight">
\[\nabla L_w = -2X^T(y-Xw) + 2\lambda w\]</div>
<p>Setting to zero and solve:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
0 &amp;= -2X^T(y-Xw) + 2\lambda w \\
&amp;= X^T(y-Xw) - \lambda w    \\
&amp;= X^Ty - X^TXw - \lambda w \\
&amp;= X^Ty - (X^TX + \lambda I_d) w
\end{align}\end{split}\]</div>
<p>Move that to other side and we get a closed-form solution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
(X^TX + \lambda I_d) w &amp;= X^Ty    \\
w &amp;= (X^TX + \lambda I_d)^{-1}X^Ty
\end{align}\end{split}\]</div>
<p>which is almost the same as linear regression without regularization.</p>
</div>
</div>
<div class="section" id="l1-penalty">
<h2>L1 Penalty<a class="headerlink" href="#l1-penalty" title="Permalink to this headline">¶</a></h2>
<p>As you might guess, you can add L1-norm for <strong>L1 regularization</strong>:</p>
<div class="math notranslate nohighlight">
\[L(w) = \sum_{i=1}^{n} \left( y^i - wx^i \right)^2 + \lambda\sum_{j=0}^{d}\left|w_j\right|\]</div>
<p>Again, in fancy term, this loss function is also known as <strong>Lasso
regression</strong>. Using matrix notation:</p>
<div class="math notranslate nohighlight">
\[L(w) = {\left\lVert y - Xw \right\rVert}^2 + \lambda{\left\lVert w \right\rVert}_1\]</div>
<p>It’s more complex to get a closed-form solution for this, so we’ll leave
it here.</p>
</div>
<div class="section" id="l1-vs-l2-penalty">
<h2>L1 vs L2 Penalty<a class="headerlink" href="#l1-vs-l2-penalty" title="Permalink to this headline">¶</a></h2>
<p>What’s the point of using different penalty terms, as it seems like both
try to push down the size of <span class="math notranslate nohighlight">\(w\)</span>.</p>
<p>Turns out L1 penalty tends to produce <em>sparse solutions</em>, which means
most entries in <span class="math notranslate nohighlight">\(w\)</span> are zeros. This is great if you want the model
to be simple and compact.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm

np.random.seed(1)

Lambda = 0.2

True_w1 = 300
True_w0 = 500
def true_target(x):
  return True_w1*x + True_w0

def observed_target(x):
  &quot;&quot;&quot;Underlying data with Gaussian noise added&quot;&quot;&quot;
  normal_noise = np.random.normal() * 6
  return true_target(x) + normal_noise

N = 50

# Features, X is [1,50]
# X = np.arange(N).reshape(N, 1)
X = np.random.rand(N).reshape(N, 1)

# Observed targets
y = np.array([observed_target(x) for x in X]).reshape(N, 1)

# Append 1 for intercept term later
X = np.hstack([np.ones((N, 1)), X])

from mpl_toolkits.mplot3d import Axes3D

# Ranges of w0 and w1 to see, centering at the true line
spanning_radius = np.max([True_w1, True_w0, 200])
step = 0.5
w0range = np.arange(True_w0-spanning_radius, True_w0+spanning_radius, step)
w1range = np.arange(True_w1-spanning_radius, True_w1+spanning_radius, step)
w0grid, w1grid = np.meshgrid(w0range, w1range)

range_len = len(w0range)
print(&quot;Number of values in each axis:&quot;, range_len)

# Make [w0, w1] in (2, 14400) shape
all_w0w1_values = np.hstack([w0grid.flatten()[:,None], w1grid.flatten()[:,None]]).T

# Compute raw losses
raw_losses = np.linalg.norm(y - (X @ all_w0w1_values), axis=0) ** 2
raw_losses = raw_losses.reshape((range_len, range_len))

# Compute L2 penalty losses
penalty_loss = Lambda * np.linalg.norm(all_w0w1_values, axis=0) ** 2
penalty_loss = penalty_loss.reshape((range_len, range_len))

from sklearn.linear_model import Ridge, Lasso, LinearRegression

lr = LinearRegression(fit_intercept=False)
lr = lr.fit(X, y)

ridge = Ridge(alpha=Lambda)
ridge = ridge.fit(X, y)

lasso = Lasso(alpha=Lambda)
lasso = lasso.fit(X, y)

ridge_coef = ridge.coef_[0]
lasso_coef = lasso.coef_
lr_coef = lr.coef_[0]

print(&quot;Ridge solution:&quot;, ridge_coef)
print(&quot;Lasso solution:&quot;, lasso_coef)
print(&quot;Linear Regression solution:&quot;, lr_coef)

ridge_loss = np.linalg.norm(y - X @ ridge_coef) ** 2 + Lambda * np.linalg.norm(ridge_coef, ord=2) ** 2
lasso_loss = np.linalg.norm(y - X @ lasso_coef) ** 2 + Lambda * np.linalg.norm(lasso_coef, ord=1)
lr_loss = np.linalg.norm(y - X @ lr_coef) ** 2

print(&quot;Ridge loss:&quot;, ridge_loss)
print(&quot;Lasso loss:&quot;, lasso_loss)
print(&quot;Linear Regression loss:&quot;, lr_loss)

fig = plt.figure(figsize=(10,6))
ax = fig.gca(projection=&#39;3d&#39;)

ax.plot_surface(w0grid, w1grid, raw_losses, alpha=0.4, cmap=&#39;RdBu&#39;)
ax.contour(w0grid, w1grid, penalty_loss, alpha=1, cmap=&#39;plasma_r&#39;)

ax.scatter([0],[0], 0, c=&#39;black&#39;, s=100, label=&quot;(0,0)&quot;)

ax.scatter([ridge_coef[0]],[ridge_coef[1]], 0, c=&#39;green&#39;, s=100, label=&quot;Ridge solution&quot;)
ax.scatter([lr_coef[0]],[lr_coef[1]], 0, c=&#39;red&#39;, s=100, label=&quot;Linear regression solution&quot;)

ax.legend(loc=&#39;best&#39;)
ax.set_xlabel(&#39;w[0]&#39;)
ax.set_ylabel(&#39;w[1]&#39;)
ax.set_zlabel(&#39;L(w)&#39;)
ax.set_xticks(np.arange(True_w0-spanning_radius, True_w0+spanning_radius, 100))
ax.set_yticks(np.arange(True_w1-spanning_radius, True_w1+spanning_radius, 100))
ax.set_zticks([ridge_loss])
# plt.axis(&#39;off&#39;)
# ax.set_zlim(0, 20000)

plt.show()
</pre></div>
</div>
<img alt="../../_images/sphx_glr_linear_regression_regularized_tutorial_001.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_linear_regression_regularized_tutorial_001.png" />
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Number of values in each axis: 2000
Ridge solution: [  0.         289.21906667]
Lasso solution: [  0.         299.48649757]
Linear Regression solution: [500.00853749 301.63252862]
Ridge loss: 680374639.5444187
Lasso loss: 669697965.9185725
Linear Regression loss: 42469207.783152446
</pre></div>
</div>
<p><strong>Total running time of the script:</strong> ( 0 minutes  5.456 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-blog-content-linear-regression-linear-regression-regularized-tutorial-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" download="" href="../../_downloads/12019720201790744867625194934f96/linear_regression_regularized_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">linear_regression_regularized_tutorial.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" download="" href="../../_downloads/b46a4fb45981c08755e62c4ed9308063/linear_regression_regularized_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">linear_regression_regularized_tutorial.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="linear_regression_tutorial.html" class="btn btn-neutral float-left" title="A Complete Guide to Matrix Notation and Linear Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, aunnnn

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>