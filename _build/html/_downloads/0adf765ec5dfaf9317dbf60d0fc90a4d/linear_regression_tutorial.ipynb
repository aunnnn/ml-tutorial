{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nA Complete Guide to Matrix Notation and Linear Regression\n=========================================================\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's really understand matrix notation in context of linear regression,\nfrom the ground up.\n\nLinear Regression finds the best line, or *hyperplane* $\\hat{y}$\nin higher dimension, or generally a function $f$:\n\n\\begin{align}\\hat{y} = f(x) = wx\\end{align}\n\nthat fits the whole data. This is just a dot product between vector\n$w$ and a data point $x$ in $d$ dimension:\n\n\\begin{align}\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_dx_d\\end{align}\n\nNotice that we use $w_0$ as an intercept term, and thus we need to\nadd a dummy dimension with value of \"1\" ($x_0$) for all data\npoints $x$. Thus, $x$ here is on $d+1$ dimension.\nThink of it as the y-intercept term $c$ in 2-dimension\n($y = mx + c$).\n\nAnother way to look at this is that $f(x)$ transforms a data point\n$x$ on $d+1$ dimension into a predicted scalar value\n$\\hat{y}$ that is close to target $y$:\n\n\\begin{align}\\begin{bmatrix}\n   x_0 \\\\\n   x_1 \\\\\n   \\vdots \\\\\n   x_d \n   \\end{bmatrix}\n   \\xrightarrow{f}\n   \\hat{y}\n   \\approx y\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Sum of Squared Error Loss\n-----------------------------\n\nThe best way to solve this is to find $w$ that minimizes the **sum\nof squared errors (SSE)**\\ $^\\dagger$, or the \"error\" between all\nof predicted value $\\hat{y}^i$ and the target $y^i$ of\n$i^{th}$ data point for $i = 1$ to $n$, writing this\nas a loss function $L(w)$:\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - \\hat{y}^i \\right)^2 = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2\\end{align}\n\nFrom now on we refer to a data point (d+1 vector) as $x^i$ and its\ncorresponding target (scalar) as $y^i$.\n\nSurprisingly, the SSE loss is not from someone's intuition, but it's\nfrom the assumption that there is **Gaussian noise in our observation**\nof the underlying linear relationship. We will show how this leads to\nSSE loss later, but first let's visualize what we're trying to do.\n\n.. figure:: ./img_lr_objective.png\n   :alt: img\n\n   img\n\n1. There is a true line, the true linear relationship that we want to\n   discover (blue line).\n2. The data points are then observed through noise **deviating from that\n   line, with Gaussian distribution**.\n3. Suppose we predict a random line (red), not the best one yet.\n4. We calculate the distance or difference between the predicted points\n   (along the line) and the actual data points. This is then **squared\n   and sum up to get sum of squared error**.\n\n    Linear regression is the method to get the line that fits the given\n    data with the minimum sum of squared error.\n\n*Note: I know it's confusing for the first time, but you'll get used to\nusing superscript for indexing data points...*\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to Find the Optimal Solution\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn optimal solution ($w$) for this equation can be found either\nusing *closed-form solution* or via iterative methods like gradient\ndescent.\n\n**A closed-form solution** means we figure out the formula for\n$w = ...$. Implementing that formula in a program directly solves\nthe problem. The thing is you have to come up with the correct one\nyourself, by hand.\n\nDo you remember how to find a minimum (or maximum) value for a function?\nWe take the derivative of the function above with respect to $w$,\nset it to zero, and solve for the $w$ in terms of other\nparameters. This is like taking a single jump to the optimal value. We\ndo all the hard work for computers.\n\nLuckily we can do this for linear regression, but not all loss functions\nbe solved this way, actually, only a few. In those cases, we use\n**iterative methods like gradient descent** to search for the solution.\nIn contrast to closed-form solution, we do not jump directly to the\noptimal answer, instead, we take many steps that lead us near to where\nthe optimal answer lives.\n\nNext let's derive the closed-form solution for linear regression. In\norder to do that efficiently, we need some matrix notations.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Going into Matrix Notation\n--------------------------\n\nWriting things down in matrix notation makes things much faster in\nNumPy. **But it's not easy to read matrix notation, especially if you\nstudy machine learning on your own.** There're things like dot product,\nmatrix multiplication, transpose and stuff that you need to keep track\nof in your head. If you're starting out, then please write them on\npapers, drawing figures as needed to make you understand. It really pays\noff.\n\nOn top of that, these few **key standards** will make our lives with\nlinear algebra easier:\n\n1. Always a *column* vector\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen you see standalone vectors in a matrix notation formula, assumes\nit's a column vector. E.g.,\n\n\\begin{align}x = \n   \\begin{bmatrix}\n   1 \\\\\n   2 \\\\\n   3 \\\\\n   \\end{bmatrix}\\end{align}\n\nand so its transpose is a row vector,\n\n\\begin{align}x^T = \n   \\begin{bmatrix}\n   1 & 2 & 3\n   \\end{bmatrix}\\end{align}\n\nLikewise, you should try to make the final result of matrix operation to\nbe a column vector.\n\nNote that the NumPy vector created by ``np.zeros``, ``np.arange``, etc.,\nis not really a column vector. It has only one dimension ``(N,)``. So,\nyou cannot transpose it directly (``x.T`` still gives you ``x``.) To\nconvert it to a column vector, we use ``x.reshape(N,1)`` or\n``x[:, None]``.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Feature matrix $X$ is rows of data points\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOur data points $x^i$ are on $d+1$ dimension, and there is\n$n$ of them, we store them all in a 2-d matrix $X$:\n\n\\begin{align}X = \\begin{align}\n   \\underset{n\\times d}\n   {\\begin{bmatrix}\n   \\longleftarrow & x^1 & \\longrightarrow \\\\\n   \\longleftarrow & x^2 & \\longrightarrow \\\\\n   & \\vdots & \\\\\n   \\longleftarrow & x^n & \\longrightarrow \\\\\n   \\end{bmatrix}}\n   \\end{align}\\end{align}\n\nEach row in $X$ is a row vector for each data point. Also note\nthat we use uppercase letter for matrix.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Again, $w$ is a column vector\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nLike the first point, our $w$ will be $d+1$ dimension column\nvector with w\\_0 as an intercept term:\n\n\\begin{align}w = \n   \\begin{bmatrix}\n   w_0 \\\\\n   w_1 \\\\\n   \\vdots \\\\\n   w_d \n   \\end{bmatrix}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Dot products of rows in matrix $X$ with vector $w$ is $Xw$\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSometimes we want the dot product of each row in matrix with a vector:\n\n\\begin{align}Xw = \\underset{n\\times (d+1)}\n   {\n       \\begin{bmatrix}\n       \\longleftarrow & x^1 & \\longrightarrow \\\\\n       \\longleftarrow & x^2 & \\longrightarrow \\\\\n       & \\vdots & \\\\\n       \\longleftarrow & x^n & \\longrightarrow \\\\\n       \\end{bmatrix}\n   }\n   \\underset{(d+1) \\times 1}\n   {\n       \\begin{bmatrix}\n       \\uparrow \\\\\n       w \\\\\n       \\downarrow\n       \\end{bmatrix}\n   }\n   =\n   \\begin{bmatrix}\n   x^1w \\\\\n   x^2w \\\\\n   \\vdots \\\\\n   x^nw\n   \\end{bmatrix}\\end{align}\n\ngiven that $X$ contains rows of vectors we want to dot product\nwith.\n\nInterestingly, this gives us a column vector of our predictions\n$\\hat{y}$:\n\n\\begin{align}\\begin{bmatrix}\n   x^1w \\\\\n   x^2w \\\\\n   \\vdots \\\\\n   x^nw\n   \\end{bmatrix} =\n   \\begin{bmatrix}\n   \\hat{y}^1 \\\\\n   \\hat{y}^2 \\\\\n   \\vdots \\\\\n   \\hat{y}^n \\\\\n   \\end{bmatrix} = \\hat{y}\\end{align}\n\nIt's also good to remind yourself that it sums along dimension of\n$x^i$ and $w$:\n\n\\begin{align}Xw =\n   \\begin{bmatrix}\n   \\sum_{j=0}^{d} x_j^1w_j \\\\\n   \\sum_{j=0}^{d} x_j^2w_j \\\\\n   \\vdots \\\\\n   \\sum_{j=0}^{d} x_j^nw_j \\\\\n   \\end{bmatrix}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Sum of Squared is $x^Tx$\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis is a useful pattern to memorize. Sometimes we want the sum of\nsquared of each element in arbitrary d-dimension vector $x$:\n\n\\begin{align}\\sum_{j=1}^{d} x_i^2\\end{align}\n\nwhich is simply $x^Tx$:\n\n\\begin{align}x^Tx = \n   \\begin{bmatrix}\n   x_1 & ... & x_d\n   \\end{bmatrix}\n   \\begin{bmatrix}\n   x_1 \\\\\n   \\vdots \\\\\n   x_d \n   \\end{bmatrix}\n   = \\sum_{j=1}^{d} x_i^2\\end{align}\n\nNotice that the result of $x^Tx$ is scalar, e.g., a number.\n\nIn fancy term,\n${\\left\\lVert x \\right\\rVert} = \\sqrt{\\sum_{j=1}^{d} x_i^2}$ is\nL2-norm (or Euclidean norm) of $x$. So we can write sum of squared\nas ${\\left\\lVert x \\right\\rVert}^2 = \\sum_{j=1}^{d} x_i^2$. For\nnow, let's not care what norm actually means.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing SSE Loss in Matrix Notation\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNow you're ready, let's write the above SSE loss function in matrix\nnotation. If you look at $L(w)$ closely, it's a sum of squared of\nvector $y - \\hat{y}$. This means we can kick-off by applying our\nfifth trick:\n\n\\begin{align}L(w) = {\\left\\lVert y - \\hat{y} \\right\\rVert}^2\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we just have to find $y - \\hat{y}$. First we encode target\n$y$ in a long column vector of shape ``[n, 1]``:\n\n\\begin{align}y = \n   \\begin{bmatrix}\n   y^1 \\\\\n   y^2 \\\\\n   \\vdots \\\\\n   y^n \n   \\end{bmatrix}\\end{align}\n\n(Remember that we use superscript for indexing the $i^{th}$ data\npoint.)\n\nNext we encode each of our predicted values ($\\hat{y}^i$) in a\ncolumn vector $\\hat{y}$. Since $\\hat{y}^i$ is a dot product\nbetween $w$ and each of $x^i$, we can apply\n`4 <#4.-Dot-products-of-rows-in-matrix-$X$-with-vector-$w$-is-$Xw$>`__:\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{align}\\begin{align}\n   y - \\hat{y} &= \n   \\begin{bmatrix}\n   y^1 \\\\\n   y^2 \\\\\n   \\vdots \\\\\n   y^n \n   \\end{bmatrix}\n   - \\begin{bmatrix}\n   \\hat{y}^1 \\\\\n   \\hat{y}^2 \\\\\n   \\vdots \\\\\n   \\hat{y}^n \n   \\end{bmatrix} && \\text{(Error between target and predicted)} \\\\ &=\n   \\begin{bmatrix}\n   y^1 \\\\\n   y^2 \\\\\n   \\vdots \\\\\n   y^n \n   \\end{bmatrix}\n   - \\begin{bmatrix}\n   x^1w \\\\\n   x^2w \\\\\n   \\vdots \\\\\n   x^nw\n   \\end{bmatrix} \n   && \\text{(Predicted is a dot product of $w$ and each of data point $x^i$)} \\\\ &=\n   \\underset{n\\times 1}\n   {\n       \\begin{bmatrix}\n       y^1 \\\\\n       y^2 \\\\\n       \\vdots \\\\\n       y^n \n       \\end{bmatrix}\n   }\n   - \\underset{n\\times (d+1)}\n   {\n       \\begin{bmatrix}\n       \\longleftarrow & x^1 & \\longrightarrow \\\\\n       \\longleftarrow & x^2 & \\longrightarrow \\\\\n       & \\vdots & \\\\\n       \\longleftarrow & x^n & \\longrightarrow \\\\\n       \\end{bmatrix}\n   }\n   \\underset{(d+1)\\times 1}\n   {\n       \\begin{bmatrix}\n       \\uparrow \\\\\n       w \\\\\n       \\downarrow\n       \\end{bmatrix}\n   } && \\text{(Separate them out)} \\\\ &=\n   y - Xw && \\text{(Encode in matrix/vector form)}\n   \\end{align}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting all together we get our loss function for linear regression:\n\n\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2\\end{align}\n\nIn NumPy code, we can compute $L(w) = (y - Xw)^T(y - Xw)$.\n\nThere's no intuitive way to come up with this nice formula the first\ntime you saw it. You have to work it out and put things together\nyourself. Then you'll start to memorize the pattern and it'll become\neasier.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deriving a Closed-form Solution\n-------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do that, we'll take derivative of $L(w)$ with respect to\n$w$, set to zero and solve for $w$.\n\nWriting matrix notation is already hard, taking derivative of it is even\nharder. I recommend writing out partial derivatives to see what happens.\nFor $L(w) = L_w$, we have to take derivative with respect to each\ndimension of $w$:\n\n\\begin{align}\\nabla L_w = \n   \\begin{bmatrix}\n   \\frac{\\partial L}{\\partial w_0} \\\\\n   \\frac{\\partial L}{\\partial w_1} \\\\\n   \\vdots \\\\\n   \\frac{\\partial L}{\\partial w_d} \\\\\n   \\end{bmatrix} \n   =\n   \\begin{bmatrix}\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_0} \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_1} \\\\\n   \\vdots \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_d}\n   \\end{bmatrix} \n   =\n   \\begin{bmatrix}\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_0} \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_1} \\\\\n   \\vdots \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_d}\n   \\end{bmatrix} \n   =\n   \\underset{(d+1) \\times 1}\n   {\n   \\begin{bmatrix}\n   -2\\sum_{i=1}^{n} x^i_0 \\left( y^i - wx^i \\right) \\\\\n   -2\\sum_{i=1}^{n} x^i_1 \\left( y^i - wx^i \\right) \\\\\n   \\vdots \\\\\n   -2\\sum_{i=1}^{n} x^i_d \\left( y^i - wx^i \\right) \\\\\n   \\end{bmatrix}\n   }\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like we might be able to apply our fourth point ($Xw$, but\nin this case $w$ is $(y - Xw)$. But unlike our fourth point,\nwe now sum along data points ($n$) instead of dimensions\n($d$). For this, we want each row of $X$ to be one given\ndimension along all data points instead of one data point with all\ndimensions, and thus we use $X^T$ instead of $X$. Finally,\nhere's the full derivative in matrix notation:\n\n\\begin{align}\\nabla L_w = -2X^T(y-Xw)\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting to zero and solve:\n\n\\begin{align}\\begin{align}\n   0 &= -2X^T(y-Xw) \\\\\n   &= X^T(y-Xw)     \\\\ \n   &= X^Ty - X^TXw \n   \\end{align}\\end{align}\n\nMove $X^TX$ to other side and we get a closed-form solution:\n\n\\begin{align}\\begin{align}\n   X^TXw &= X^Ty    \\\\\n   w &= (X^TX)^{-1}X^Ty\n   \\end{align}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In NumPy, this is:\n\n.. code:: python\n\n    w = np.linalg.inv(X.T @ X) @ X @ y\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A NumPy Example\n---------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create a fake dataset from the underlying equation\n$y = 2x + 7$:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def true_target(x):\n  return 2*x + 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practical settings, there is no way we know this exact equation. We\nonly get **observed** targets, and there's some **noise** on it. The\nreason is that it's impossible to measure any data out there in the\nworld perfectly:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def observed_target(x):\n  \"\"\"Underlying data with Gaussian noise added\"\"\"\n  normal_noise = np.random.normal() * 3\n  return true_target(x) + normal_noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating data points\n~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, make 50 data points, observations and targets:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 50\n\n# Features, X is [1,50]\nX = np.random.rand(N).reshape(N, 1) * 10\n\n# Observed targets\ny = np.array([observed_target(x) for x in X]).reshape(N, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding dummy dimension term to each $x^i$:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Append 1 for intercept term later\nX = np.hstack([np.ones((N, 1)), X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it **doesn't matter** here whether we add it to the front or\nback, it will simply reflect correspondingly in our solution $w$.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize our data points with respect to the true line\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For plotting\nfeatures = X[:,1:] # exclude the intercept for plotting\ntarget = y\ntrue_targets = true_target(X[:,1:])\n\nplt.scatter(features, target, s=10, label='Observed data points')\nplt.plot(features, true_targets, c='blue', label='True target line y = 2x + 7', alpha=0.3)\n\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend(loc='best')\nplt.title('True and observed data points')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute a closed-form solution\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to get the line that is closest to that true target (blue)\nline as possible, without the knowledge of its existence. For this we\nuse linear regression to fit observed data points by following the\nformula from the previous section:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "w = np.linalg.inv(X.T @ X) @ X.T @ y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To predict, we compute $\\hat{y} = xw$ for each data point\n$x^i$. Here we predict the training set (``X``) itself:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted = X @ w # y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To predict a set of new points, you just make it the same format as\n``X``, e.g., rows of data points.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize best fit line vs. true target line\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(features, target, s=10, label='Data points')\nplt.plot(features, true_targets, c='blue', label='True target line', alpha=0.3)\nplt.plot(features, predicted, c='red', label='Best fit line')\n\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend(loc='best')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's pretty close.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the result\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nAnd our $w$ is:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we append ones in front of each data point $x$, ``w[0]``\nwill be the intercept term and ``w[1]`` will be the slope. So our\npredicted line will be in the format of ``y = w[1]*x + w[0]``. Recall\nthe *true* equation $y = 2x + 7$, you can see that we almost got\nthe true slope (2):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Our slope is\", w[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The intercept seems a little off, but that's okay because our data is in\na big range ($x \\in [0, 50], y \\in [7, 107]$). If we normalize the\ndata into $[0, 1]$ range, expect it to be much closer.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is our sum of squared error for the best fit line. Note that the\nnumber doesn't mean anything much, apart from that this is the least\npossible loss we would get from any lines that try to fit the data:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diff = (y - X @ w)\nloss = diff.T @ diff\nprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you don't want intermediate variable, you can use ``np.linalg.norm``,\nbut to get the sum of squared loss, you have to square that after:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = np.linalg.norm(y - X @ w, ord=2) ** 2\nprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the loss surface\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLet's confirm that our solution is really the one with lowest loss by\nseeing the loss surface.\n\nOur loss function $L(w)$ depends on two dimensions of $w$,\ne.g., ``w[0]`` and ``w[1]``. If we plot $L(w)$ over possible\nvalues of ``w[0]`` and ``w[1]``, the minimum of $L(w)$ should be\nnear ``w = [5.17,2.066]``, which is our solution.\n\nTo plot that out, first we have to create all possible values of w[0]\nand w[1] in a grid.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n\n# Ranges of w0 and w1 to see, centering at the true line\nspanning_radius = 10\nw0range = np.arange(7-spanning_radius, 7+spanning_radius, 0.05)\nw1range = np.arange(2-spanning_radius, 2+spanning_radius, 0.05)\nw0grid, w1grid = np.meshgrid(w0range, w1range)\n\nrange_len = len(w0range)\nprint(\"Number of values in each axis:\", range_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means we'll look into a total of 400\\*400 = 160,000 values of\n``w``. We have to calculate loss for each pair of ``w0, w1``:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make [w0, w1] in (2, 14400) shape\nall_w0w1_values = np.hstack([w0grid.flatten()[:,None], w1grid.flatten()[:,None]]).T\n\n# Compute all losses, reshape back to grid format\nall_losses = (np.linalg.norm(y - (X @ all_w0w1_values), axis=0, ord=2) ** 2).reshape((range_len, range_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can plot the loss surface (with minimum at the red point):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,6))\nax = fig.gca(projection='3d')\n\nax.plot_surface(w0grid, w1grid, all_losses, alpha=0.5, cmap='RdBu')\nax.contour(w0grid, w1grid, all_losses, offset=0, alpha=1, cmap='RdBu')\nax.scatter(w[0], w[1], loss, lw=3, c='red', s=100, label=\"Minimum point (5.9,2.2)\")\n\nax.legend(loc='best')\nax.set_xlabel('w[0]')\nax.set_ylabel('w[1]')\nax.set_zlabel('L(w)')\nax.set_xticks(np.arange(7-spanning_radius, 7+spanning_radius, 2))\nax.set_yticks(np.arange(2-spanning_radius, 2+spanning_radius, 2))\nax.set_zticks([loss])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can notice the bowl **centers** at the solution.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using sklearn\n-------------\n\nUsing ``sklearn`` for linear regression is very simple (if you already\nunderstand all the concepts above).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we create the classifier ``clf``. If ``fit_intercept`` is ``True``\n(default), then it adds the dummy '1' to the ``X``. But we already did\nthat manually, so set it to ``False`` here.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = LinearRegression(fit_intercept=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then fit the data:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf.fit(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the $w$ learned, it's the same as ours:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(clf.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Previously we use ``X @ w`` to predict data. For ``sklearn`` we can use\n``clf.predict``:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted = clf.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the result is the same:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.scatter(X[:,1:], y, s=10, label='Data points')\nplt.plot(X[:,1:], true_targets, c='blue', label='True line', alpha=0.3)\nplt.plot(X[:,1:], predicted, c='red', label='Best fit line')\nplt.legend(loc='best')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}