{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nA Complete Guide to Matrix Notation and Linear Regression\n=========================================================\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's really understand matrix notation in context of linear regression,\nfrom the ground up.\n\nLinear Regression finds the best line, or *hyperplane* $\\hat{y}$\nin higher dimension, or generally a function $f$:\n\n\\begin{align}\\hat{y} = f(x) = wx\\end{align}\n\nthat fits the whole data. This is just a dot product between vector\n$w$ and a data point $x$ in $d$ dimension:\n\n\\begin{align}\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_dx_d\\end{align}\n\nNotice that we use $w_0$ as an intercept term, and thus we need to\nadd a dummy dimension with value of \"1\" ($x_0$) for all data\npoints $x$. Thus, $x$ here is on $d+1$ dimension.\nThink of it as the y-intercept term $c$ in 2-dimension\n($y = mx + c$).\n\nAnother way to look at this is that $f(x)$ transforms a data point\n$x$ on $d+1$ dimension into a predicted scalar value\n$\\hat{y}$ that is close to target $y$:\n\n\\begin{align}\\begin{bmatrix}\n   x_0 \\\\\n   x_1 \\\\\n   \\vdots \\\\\n   x_d \n   \\end{bmatrix}\n   \\xrightarrow{f}\n   \\hat{y}\n   \\approx y\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Sum of Squared Error Loss\n-----------------------------\n\nThe best way to solve this is to find $w$ that minimizes the **sum\nof squared errors (SSE)**\\ $^\\dagger$, or the \"error\" between all\nof predicted value $\\hat{y}^i$ and the target $y^i$ of\n$i^{th}$ data point for $i = 1$ to $n$, writing this\nas a loss function $L(w)$:\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - \\hat{y}^i \\right)^2 = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2\\end{align}\n\nFrom now on we refer to a data point (d+1 vector) as $x^i$ and\ntarget (scalar) as $y^i$.\n\n*$^\\dagger$ Surprisingly, the SSE loss is not from someone's\nintuition, but it's systematically derived from the assumption that\nthere is Gaussian noise with our observation of the underlying linear\nrelationship.*\n\nHow to Find the Optimal Solution\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn optimal solution ($w$) for this equation can be found either\nusing *closed-form solution* or via iterative methods like gradient\ndescent.\n\n**A closed-form solution** means we figure out the formula for\n$w = ...$. Implementing that formula in a program directly solves\nthe problem. The thing is you have to come up with the correct one\nyourself, by hand.\n\nDo you remember how to find a minimum (or maximum) value for a function?\nWe take the derivative of the function above with respect to $w$,\nset it to zero, and solve for the $w$ in terms of other\nparameters. This is like taking a single jump to the optimal value. We\ndo all the hard work for computers.\n\nLuckily we can do this for linear regression, but not all loss functions\nbe solved this way, actually, only a few. In those cases, we use\n**iterative methods like gradient descent** to search for the solution.\nIn contrast to closed-form solution, we do not jump directly to the\noptimal answer, instead, we take many steps that lead us near to where\nthe optimal answer lives.\n\nNext let's derive the closed-form solution for linear regression. In\norder to do that efficiently, we need some matrix notations.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Going into Matrix Notation\n--------------------------\n\nWriting things down in matrix notation makes things much faster in\nNumPy. **But it's not easy to read matrix notation, especially if you\nstudy machine learning on your own.** There're things like dot product,\nmatrix multiplication, transpose and stuff that you need to keep track\nof in your head. If you're starting out, then please write them on\npapers, drawing figures as needed to make you understand. It really pays\noff.\n\nOn top of that, these few **key standards** will make our lives with\nlinear algebra easier:\n\n1. Always a *column* vector\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen you see standalone vectors in a matrix notation formula, assumes\nit's a column vector. E.g.,\n\n\\begin{align}x = \n   \\begin{bmatrix}\n   1 \\\\\n   2 \\\\\n   3 \\\\\n   \\end{bmatrix}\\end{align}\n\nand so its transpose is a row vector,\n\n\\begin{align}x^T = \n   \\begin{bmatrix}\n   1 & 2 & 3\n   \\end{bmatrix}\\end{align}\n\nLikewise, you should try to make the final result of matrix operation to\nbe a column vector.\n\nNote that the NumPy vector created by ``np.zeros``, ``np.arange``, etc.,\nis not really a column vector. It has only one dimension ``(N,)``. So,\nyou cannot transpose it directly (``x.T`` still gives you ``x``.) To\nconvert it to a column vector, we use ``x.reshape(N,1)`` or\n``x[:, None]``.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Feature matrix $X$ is rows of data points\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOur data points $x^i$ are on $d+1$ dimension, and there is\n$n$ of them, we store them all in a 2-d matrix $X$:\n\n\\begin{align}X = \\begin{align}\n   \\underset{n\\times d}\n   {\\begin{bmatrix}\n   \\longleftarrow & x^1 & \\longrightarrow \\\\\n   \\longleftarrow & x^2 & \\longrightarrow \\\\\n   & \\vdots & \\\\\n   \\longleftarrow & x^n & \\longrightarrow \\\\\n   \\end{bmatrix}}\n   \\end{align}\\end{align}\n\nEach row in $X$ is a row vector for each data point. Also note\nthat we use uppercase letter for matrix.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Again, $w$ is a column vector\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nLike the first point, our $w$ will be $d+1$ dimension column\nvector with w\\_0 as an intercept term:\n\n\\begin{align}w = \n   \\begin{bmatrix}\n   w_0 \\\\\n   w_1 \\\\\n   \\vdots \\\\\n   w_d \n   \\end{bmatrix}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Dot products of rows in matrix $X$ with vector $w$ is $Xw$\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSometimes we want the dot product of each row in matrix with a vector:\n\n\\begin{align}Xw = \\underset{n\\times (d+1)}\n   {\n       \\begin{bmatrix}\n       \\longleftarrow & x^1 & \\longrightarrow \\\\\n       \\longleftarrow & x^2 & \\longrightarrow \\\\\n       & \\vdots & \\\\\n       \\longleftarrow & x^n & \\longrightarrow \\\\\n       \\end{bmatrix}\n   }\n   \\underset{(d+1) \\times 1}\n   {\n       \\begin{bmatrix}\n       \\uparrow \\\\\n       w \\\\\n       \\downarrow\n       \\end{bmatrix}\n   }\n   =\n   \\begin{bmatrix}\n   x^1w \\\\\n   x^2w \\\\\n   \\vdots \\\\\n   x^nw\n   \\end{bmatrix}\\end{align}\n\ngiven that $X$ contains rows of vectors we want to dot product\nwith.\n\nInterestingly, this gives us a column vector of our predictions\n$\\hat{y}$:\n\n\\begin{align}\\begin{bmatrix}\n   x^1w \\\\\n   x^2w \\\\\n   \\vdots \\\\\n   x^nw\n   \\end{bmatrix} =\n   \\begin{bmatrix}\n   \\hat{y}^1 \\\\\n   \\hat{y}^2 \\\\\n   \\vdots \\\\\n   \\hat{y}^n \\\\\n   \\end{bmatrix} = \\hat{y}\\end{align}\n\nIt's also good to remind yourself that it sums along dimension of\n$x^i$ and $w$:\n\n\\begin{align}Xw =\n   \\begin{bmatrix}\n   \\sum_{j=0}^{d} x_j^1w_j \\\\\n   \\sum_{j=0}^{d} x_j^2w_j \\\\\n   \\vdots \\\\\n   \\sum_{j=0}^{d} x_j^nw_j \\\\\n   \\end{bmatrix}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Sum of Squared is $x^Tx$\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis is a useful pattern to memorize. Sometimes we want the sum of\nsquared of each element in arbitrary d-dimension vector $x$:\n\n\\begin{align}\\sum_{j=1}^{d} x_i^2\\end{align}\n\nwhich is simply $x^Tx$:\n\n\\begin{align}x^Tx = \n   \\begin{bmatrix}\n   x_1 & ... & x_d\n   \\end{bmatrix}\n   \\begin{bmatrix}\n   x_1 \\\\\n   \\vdots \\\\\n   x_d \n   \\end{bmatrix}\n   = \\sum_{j=1}^{d} x_i^2\\end{align}\n\nNotice that the result of $x^Tx$ is scalar, e.g., a number.\n\nIn fancy term,\n${\\left\\lVert x \\right\\rVert} = \\sqrt{\\sum_{j=1}^{d} x_i^2}$ is\nL2-norm (or Euclidean norm) of $x$. So we can write sum of squared\nas ${\\left\\lVert x \\right\\rVert}^2 = \\sum_{j=1}^{d} x_i^2$. For\nnow, let's not care what norm actually means.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing SSE Loss in Matrix Notation\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNow you're ready, let's write the above SSE loss function in matrix\nnotation. If you look at $L(w)$ closely, it's a sum of squared of\nvector $y - \\hat{y}$. This means we can kick-off by applying our\nfourth trick:\n\n\\begin{align}L(w) = {\\left\\lVert y - \\hat{y} \\right\\rVert}^2\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we just have to find $y - \\hat{y}$. First we encode target\n$y$ in a long column vector of shape ``[n, 1]``:\n\n\\begin{align}y = \n   \\begin{bmatrix}\n   y^1 \\\\\n   y^2 \\\\\n   \\vdots \\\\\n   y^n \n   \\end{bmatrix}\\end{align}\n\n(Remember that we use superscript for indexing the $i^{th}$ data\npoint.)\n\nNext we encode each of our predicted values ($\\hat{y}^i$) in a\ncolumn vector $\\hat{y}$. Since $\\hat{y}^i$ is a dot product\nbetween $w$ and each of $x^i$, we can apply\n`4 <#4.-Dot-products-of-rows-in-matrix-$X$-with-vector-$w$-is-$Xw$>`__:\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{align}\\begin{align}\n   y - \\hat{y} &= \n   \\begin{bmatrix}\n   y^1 \\\\\n   y^2 \\\\\n   \\vdots \\\\\n   y^n \n   \\end{bmatrix}\n   - \\begin{bmatrix}\n   \\hat{y}^1 \\\\\n   \\hat{y}^2 \\\\\n   \\vdots \\\\\n   \\hat{y}^n \n   \\end{bmatrix} && \\text{(Error between target and predicted)} \\\\ &=\n   \\begin{bmatrix}\n   y^1 \\\\\n   y^2 \\\\\n   \\vdots \\\\\n   y^n \n   \\end{bmatrix}\n   - \\begin{bmatrix}\n   x^1w \\\\\n   x^2w \\\\\n   \\vdots \\\\\n   x^nw\n   \\end{bmatrix} \n   && \\text{(Predicted is a dot product of $w$ and each of data point $x^i$)} \\\\ &=\n   \\underset{n\\times 1}\n   {\n       \\begin{bmatrix}\n       y^1 \\\\\n       y^2 \\\\\n       \\vdots \\\\\n       y^n \n       \\end{bmatrix}\n   }\n   - \\underset{n\\times (d+1)}\n   {\n       \\begin{bmatrix}\n       \\longleftarrow & x^1 & \\longrightarrow \\\\\n       \\longleftarrow & x^2 & \\longrightarrow \\\\\n       & \\vdots & \\\\\n       \\longleftarrow & x^n & \\longrightarrow \\\\\n       \\end{bmatrix}\n   }\n   \\underset{(d+1)\\times 1}\n   {\n       \\begin{bmatrix}\n       \\uparrow \\\\\n       w \\\\\n       \\downarrow\n       \\end{bmatrix}\n   } && \\text{(Separate them out)} \\\\ &=\n   y - Xw && \\text{(Encode in matrix/vector form)}\n   \\end{align}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting all together we get our loss function for linear regression:\n\n\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2\\end{align}\n\nIn NumPy code, we can compute $L(w) = (y - Xw)^T(y - Xw)$.\n\nThere's no intuitive way to come up with this nice formula the first\ntime you saw it. You have to work it out and put things together\nyourself. Then you'll start to memorize the pattern and it'll become\neasier.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deriving a Closed-form Solution\n-------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll take derivative of $L(w)$ with respect to $w$, set to\nzero and solve for $w$.\n\nWriting matrix notation is already hard, taking derivative of it is even\nharder. I recommend writing out partial derivatives to see what happens.\nFor $L(w) = L_w$, we have to take derivative with respect to each\ndimension of $w$:\n\n\\begin{align}\\nabla L_w = \n   \\begin{bmatrix}\n   \\frac{\\partial L}{\\partial w_0} \\\\\n   \\frac{\\partial L}{\\partial w_1} \\\\\n   \\vdots \\\\\n   \\frac{\\partial L}{\\partial w_d} \\\\\n   \\end{bmatrix} \n   =\n   \\begin{bmatrix}\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_0} \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_1} \\\\\n   \\vdots \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_d}\n   \\end{bmatrix} \n   =\n   \\begin{bmatrix}\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_0} \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_1} \\\\\n   \\vdots \\\\\n   \\frac{\\partial \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2}{\\partial w_d}\n   \\end{bmatrix} \n   =\n   \\underset{(d+1) \\times 1}\n   {\n   \\begin{bmatrix}\n   -2\\sum_{i=1}^{n} x^i_0 \\left( y^i - wx^i \\right) \\\\\n   -2\\sum_{i=1}^{n} x^i_1 \\left( y^i - wx^i \\right) \\\\\n   \\vdots \\\\\n   -2\\sum_{i=1}^{n} x^i_d \\left( y^i - wx^i \\right) \\\\\n   \\end{bmatrix}\n   }\\end{align}\n\nLooks like we might be able to apply our fourth point ($Xw$, but\nin this case $w$ is $(y - Xw)$. But unlike our fourth point,\nwe now sum along data points ($n$) instead of dimensions\n($d$). For this, we want each row of $X$ to be one given\ndimension along all data points instead of one data point with all\ndimensions, and thus we use $X^T$ instead of $X$. Finally,\nhere's the full derivative in matrix notation:\n\n\\begin{align}\\nabla L_w = -2X^T(y-Xw)\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting to zero and solve:\n\n\\begin{align}\\begin{align}\n   0 &= -2X^T(y-Xw) \\\\\n   &= X^T(y-Xw)     \\\\ \n   &= X^Ty - X^TXw \n   \\end{align}\\end{align}\n\nMove $X^TX$ to other side and we get a closed-form solution:\n\n\\begin{align}\\begin{align}\n   X^TXw &= X^Ty    \\\\\n   w &= (X^TX)^{-1}X^Ty\n   \\end{align}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In NumPy, this is:\n\n.. code:: python\n\n    w = np.linalg.inv(X.T @ X) @ X @ y\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A NumPy Example\n---------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create a fake dataset from the underlying equation\n$y = 2x + 7$:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def true_target(x):\n  return 2*x + 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practical settings, there is no way we know this exact equation. We\nonly get **observed** targets, and there's some **noise** on it. The\nreason is that it's impossible to measure any data out there in the\nworld perfectly:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def observed_target(x):\n  \"\"\"Underlying data with Gaussian noise added\"\"\"\n  normal_noise = np.random.normal() * 8\n  return true_target(x) + normal_noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, make 50 data points, observations and targets:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 50\n\n# Features, X is [1,50]\nX = np.arange(N).reshape(N, 1)\n\n# Observed targets\ny = np.array([observed_target(x) for x in X]).reshape(N, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding dummy dimension term to each $x^i$:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Append 1 for intercept term later\nX = np.hstack([np.ones((N, 1)), X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it **doesn't matter** here whether we add it to the front or\nback, it will simply reflect correspondingly in our solution $w$.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the data:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For plotting\nfeatures = X[:,1:] # exclude the intercept for plotting\ntarget = y\ntrue_targets = true_target(X[:,1:])\n\nplt.scatter(features, target, s=10, label='Observed data points (noisy)')\nplt.plot(features, true_targets, c='blue', label='True target line y = 2x + 7', alpha=0.3)\n\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend(loc='best')\nplt.title('True and observed data points')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to get the line that is closest to the true target line as\npossible. For this we use linear regression with our closed-form\nsolution:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "w = np.linalg.inv(X.T @ X) @ X.T @ y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To predict, we compute $\\hat{y} = xw$ for each data point\n$x^i$:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted = X @ w # y_hat\n\nplt.scatter(features, target, s=10, label='Data points')\nplt.plot(features, true_targets, c='blue', label='True target line', alpha=0.3)\nplt.plot(features, predicted, c='red', label='Best fit line')\n\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.legend(loc='best')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's pretty close. Let see our SSE loss for this line:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sse_loss = np.linalg.norm(y - X@w, ord=2) ** 2 # Use L-2 norm from np.linalg, or do (y - X@w).T @ (y - X@w)\nprint(\"Sum of squared error is\", sse_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And our $w$ is:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we append ones in front of each data point $x$, ``w[0]``\nwill be the intercept term and ``w[1]`` will be the slope. Recall the\ntrue equation $y = 2x + 7$, you can see that we almost got the\ntrue slope (2):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(w[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The intercept seems a little off, but that's okay because our data is in\na big range ($x \\in [0, 50], y \\in [7, 107]$). If we normalize the\ndata into $[0, 1]$ range, expect it to be much closer.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}