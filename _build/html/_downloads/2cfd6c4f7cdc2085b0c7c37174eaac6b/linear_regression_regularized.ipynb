{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLinear Regression with Regularization\n=====================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regularization is a way to prevent overfitting and allows the model to\ngeneralize better. We\u2019ll cover the *Ridge* and *Lasso* regression here.\n\nThe Need for Regularization\n---------------------------\n\nUnlike polynomial fitting, it\u2019s hard to imagine how linear regression\ncan overfit the data, since it\u2019s just a single line (or a hyperplane).\nOne situation is that features are **correlated** or redundant.\n\nSuppose there are two features, both are exactly the same, our predicted\nhyperplane will be in this format:\n\n\\begin{align}\\hat{y} = w_0 + w_1x_1 + w_2x_2\\end{align}\n\nand the true values of $x_2$ is almost the same as $x_1$ (or\nwith some multiplicative factor and noise). Then, it\u2019s best to just drop\n$w_2x_2$ term and use:\n\n\\begin{align}\\hat{y} = w_0 + w_1x_1\\end{align}\n\nto fit the data. This is a simpler model.\n\nBut we don\u2019t know whether $x_1$ and $x_2$ is **actually**\nredundant or not, at least with bare eyes, and we don\u2019t want to manually\ndrop a parameter just because we feel like it. We want to model to learn\nto do this itself, that is, to *prefer a simpler model that fits the\ndata well enough*.\n\nTo do this, we add a *penalty term* to our loss function. Two common\npenalty terms are L2 and L1 norm of $w$.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L2 and L1 Penalty\n-----------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0. No Penalty (or Linear)\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis is linear regression without any regularization (from `previous\narticle </blog_content/linear_regression/linear_regression_tutorial.html#writing-sse-loss-in-matrix-notation>`__):\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. L2 Penalty (or Ridge)\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe can add the **L2 penalty term** to it, and this is called **L2\nregularization**.:\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2 + \\lambda\\sum_{j=0}^{d}w_j^2\\end{align}\n\nThis is called L2 penalty just because it\u2019s a L2-norm of $w$. In\nfancy term, this whole loss function is also known as **Ridge\nregression**.\n\nLet\u2019s see what\u2019s going on. Loss function is something we **minimize**.\nAny terms that we add to it, we also want it to be minimized (that\u2019s why\nit\u2019s called *penalty term*). The above means we want $w$ that fits\nthe data well (first term), but we also want the values of $w$ to\nbe small as possible (second term). The lambda ($\\lambda$) is\nthere to adjust how much to penalize $w$. Note that ``sklearn``\nrefers to this as alpha ($\\alpha$) instead, but whatever.\n\nIt\u2019s tricky to know the appropriate value for lambda. You just have to\ntry them out, in exponential range (0.01, 0.1, 1, 10, etc), then select\nthe one that has the lowest loss on validation set, or doing k-fold\ncross validation.\n\nSetting $\\lambda$ to be very low means we don\u2019t penalize the\ncomplex model much. Setting it to $0$ is the original linear\nregression. Setting it high means we strongly prefer simpler model, at\nthe cost of how well it fits the data.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Closed-form solution of Ridge\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIt\u2019s not hard to find a closed-form solution for Ridge, first write the\nloss function in matrix notation:\n\n\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2 + \\lambda{\\left\\lVert w \\right\\rVert}_2^2\\end{align}\n\nThen the gradient is:\n\n\\begin{align}\\nabla L_w = -2X^T(y-Xw) + 2\\lambda w\\end{align}\n\nSetting to zero and solve:\n\n\\begin{align}\\begin{align}\n   0 &= -2X^T(y-Xw) + 2\\lambda w \\\\\n   &= X^T(y-Xw) - \\lambda w    \\\\ \n   &= X^Ty - X^TXw - \\lambda w \\\\\n   &= X^Ty - (X^TX + \\lambda I_d) w\n   \\end{align}\\end{align}\n\nMove that to other side and we get a closed-form solution:\n\n\\begin{align}\\begin{align}\n   (X^TX + \\lambda I_d) w &= X^Ty    \\\\\n   w &= (X^TX + \\lambda I_d)^{-1}X^Ty\n   \\end{align}\\end{align}\n\nwhich is almost the same as linear regression without regularization.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. L1 Penalty (or Lasso)\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs you might guess, you can also use L1-norm for **L1 regularization**:\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2 + \\lambda\\sum_{j=0}^{d}\\left|w_j\\right|\\end{align}\n\nAgain, in fancy term, this loss function is also known as **Lasso\nregression**. Using matrix notation:\n\n\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2 + \\lambda{\\left\\lVert w \\right\\rVert}_1\\end{align}\n\nIt\u2019s more complex to get a closed-form solution for this, so we\u2019ll leave\nit here.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the Loss Surface with Regularization\n------------------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s see what these penalty terms mean geometrically.\n\nL2 loss surface\n~~~~~~~~~~~~~~~\n\n.. raw:: html\n\n   <center>\n\n|image0|\n\n.. raw:: html\n\n   </center>\n\nThis simply follows the 3D equation:\n\n\\begin{align}L(w) = {\\left\\lVert w \\right\\rVert}_2^2 = w_0^2 + w_1^2\\end{align}\n\nThe center of the bowl is lowest, since ``w = [0,0]``, but that is not\neven a line and it won\u2019t predict anything useful.\n\nL2 loss surface under different lambdas\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen you multiply the L2 norm function with lambda,\n$L(w) = \\lambda(w_0^2 + w_1^2)$, the width of the bowl changes.\nThe lowest (and flattest) one has lambda of 0.25, which you can see it\npenalizes The two subsequent ones has lambdas of 0.5 and 1.0.\n\n.. raw:: html\n\n   <center>\n\n|image1|\n\n.. raw:: html\n\n   </center>\n\nL1 loss surface\n~~~~~~~~~~~~~~~\n\nBelow is the loss surface of L1 penalty:\n\n.. raw:: html\n\n   <center>\n\n|image2|\n\n.. raw:: html\n\n   </center>\n\nSimilarly the equation is\n$L(w) = \\lambda(\\left| w_0 \\right| + \\left| w_1 \\right|)$.\n\nContour of different penalty terms\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf the L2 norm is 1, you get a unit circle ($w_0^2 + w_1^2 = 1$).\nIn the same manner, you get \u201cunit\u201d shapes in other norms:\n\n.. raw:: html\n\n   <center>\n\n|image3|\n\n.. raw:: html\n\n   </center>\n\n**When you walk along these lines, you get the same loss, which is 1**\n\nThese shapes can hint us different behaviors of each norm, which brings\nus to the next question.\n\n.. |image0| image:: imgs/img_l2_surface.png\n.. |image1| image:: imgs/img_l2_surface_lambdas.png\n.. |image2| image:: imgs/img_l1_surface.png\n.. |image3| image:: imgs/img_penalty_contours.png\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which one to use, L1 or L2?\n---------------------------\n\nWhat\u2019s the point of using different penalty terms, as it seems like both\ntry to push down the size of $w$.\n\n**Turns out L1 penalty tends to produce sparse solutions**. This means\nmany entries in $w$ are zeros. This is good if you want the model\nto be simple and compact. Why is that?\n\nGeometrical Explanation\n~~~~~~~~~~~~~~~~~~~~~~~\n\n*Note: these figures are generated with unusually high lambda to\nexaggerate the plot*\n\nFirst let\u2019s bring both linear regression and penalty loss surface\ntogether (left), and recall that we want to find the **minimum loss when\nboth surfaces are summed up** (right):\n\n.. raw:: html\n\n   <center>\n\n|image0|\n\n.. raw:: html\n\n   </center>\n\nRidge regression is like finding the middle point where the loss of a\nsum between linear regression and L2 penalty loss is lowest:\n\n.. raw:: html\n\n   <center>\n\n|image1|\n\n.. raw:: html\n\n   </center>\n\nYou can imagine starting with the linear regression solution (red point)\nwhere the loss is the lowest, then you move towards the origin (blue\npoint), where the penalty loss is lowest. **The more lambda you set, the\nmore you\u2019ll be drawn towards the origin, since you penalize the values\nof $w_i$ more** so it wants to get to where they\u2019re all zeros:\n\n.. raw:: html\n\n   <center>\n\n|image2|\n\n.. raw:: html\n\n   </center>\n\nSince the loss surfaces of linear regression and L2 norm are both\nellipsoid, the solution found for Ridge regression **tends to be\ndirectly between both solutions**. Notice how the summed ellipsoid is\nstill right in the middle.\n\n--------------\n\nFor Lasso:\n\n.. raw:: html\n\n   <center>\n\n|image3|\n\n.. raw:: html\n\n   </center>\n\nAnd this is the Lasso solution for lambda = 30 and 60:\n\n.. raw:: html\n\n   <center>\n\n|image4| |You can see it tends to drift to a corner.|\n\n.. raw:: html\n\n   </center>\n\nNotice that the ellipsoid of linear regression **approaches, and finally\nhits a corner of L1 loss**, and will always stay at that corner. What\ndoes a corner of L1 norm means in this situation? It means\n$w_1 = 0$.\n\nAgain, this is because the contour lines **at the same loss value** of\nL2 norm reaches out much farther than L1 norm:\n\n.. raw:: html\n\n   <center>\n\n|image6|\n\n.. raw:: html\n\n   </center>\n\nIf the linear regression finds an optimal contact point along the L2\ncircle, then it will stop since there\u2019s no use to move sideways where\nthe loss is usually higher. However, with L1 penalty, it can drift\ntoward a corner, because it\u2019s **the same loss along the line** anyway (I\nmean, why not?) and thus is exploited, if the opportunity arises.\n\n.. |image0| image:: imgs/img_ridge_regression.png\n.. |image1| image:: imgs/img_ridge_sol_30.png\n.. |image2| image:: imgs/img_ridge_sol_60.png\n.. |image3| image:: imgs/img_lasso_regression.png\n.. |image4| image:: imgs/img_lasso_sol_30.png\n.. |You can see it tends to drift to a corner.| image:: imgs/img_lasso_sol_60.png\n.. |image6| image:: imgs/img_l1_vs_l2_contour.png\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}