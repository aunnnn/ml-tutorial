{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLinear Regression with Regularization\n=====================================\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regularization is a way to prevent overfitting and allows the model to\ngeneralize better.\n\nUnlike polynomial fitting, it's hard to imagine how linear regression\ncan overfit the data, since it's just a single line (or a hyperplane).\nOne situation might be that features are **correlated** or redundant.\nSuppose there are two features, both are exactly the same, our predicted\nhyperplane will be in this format:\n\n\\begin{align}\\hat{y} = w_0 + w_1x_1 + w_2x_2\\end{align}\n\nand the true values of $x_2$ is almost the same as $x_1$ (or\nwith some multiplicative factor and noise). Then, it's best to just drop\n$w_2x_2$ term and use:\n\n\\begin{align}\\hat{y} = w_0 + w_1x_1\\end{align}\n\nto fit the data. This is a simpler model.\n\nBut we don't know whether $x_1$ and $x_2$ is **actually**\nredundant or not, at least with bare eyes, and we don't want to manually\ndrop a parameter just because we feel like it. We want to model to learn\nto do this itself, that is, to *prefer a simpler model that fits the\ndata well enough*.\n\nTo do this, we add a penalty term to our loss function. Two common\npenalty terms are L1 and L2.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L2 Penalty\n~~~~~~~~~~\n\nRecall the loss function of linear regression from `previous\narticle </blog_content/linear_regression/linear_regression_tutorial.html#writing-sse-loss-in-matrix-notation>`__.\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2\\end{align}\n\nwe can add the **L2 penalty term** to it, and this is called **L2\nregularization**.:\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2 + \\lambda\\sum_{j=0}^{d}w_j^2\\end{align}\n\nThis is called L2 penalty just because it's a L2-norm of $w$. In\nfancy term, this whole loss function is also known as **Ridge\nregression**.\n\nLet's see what's going on. Loss function is something we **minimize**.\nAny terms that we add to it, we also want it to be minimized (that's why\nit's called *penalty term*). The above means we want $w$ that fits\nthe data well (first term), but we also want the values of $w$ to\nbe small as possible (second term). $\\lambda$ (or whatever greek\nsigns used) is to adjust how much to penalize $w$. It's impossible\nto know the appropriate value for lambda. You just have to try them out,\nin exponential range (0.01, 0.1, 1, 10, etc), then select the one that\nhas the lowest loss on validation set, or doing k-fold cross validation.\n\nSetting $\\lambda$ to be very low means we don't penalize the\ncomplex model much. Setting it to $0$ is the vanilla linear\nregression. Setting it high means we strongly prefer simpler model, at\nthe cost of how well it fits the data.\n\nClosed-form solution\n^^^^^^^^^^^^^^^^^^^^\n\nWe can also write L2 regression in a nice matrix notation:\n\n\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2 + \\lambda{\\left\\lVert w \\right\\rVert}_2^2\\end{align}\n\nThen the gradient is:\n\n\\begin{align}\\nabla L_w = -2X^T(y-Xw) + 2\\lambda w\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting to zero and solve:\n\n\\begin{align}\\begin{align}\n   0 &= -2X^T(y-Xw) + 2\\lambda w \\\\\n   &= X^T(y-Xw) - \\lambda w    \\\\ \n   &= X^Ty - X^TXw - \\lambda w \\\\\n   &= X^Ty - (X^TX + \\lambda I_d) w\n   \\end{align}\\end{align}\n\nMove that to other side and we get a closed-form solution:\n\n\\begin{align}\\begin{align}\n   (X^TX + \\lambda I_d) w &= X^Ty    \\\\\n   w &= (X^TX + \\lambda I_d)^{-1}X^Ty\n   \\end{align}\\end{align}\n\nwhich is almost the same as linear regression without regularization.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L1 Penalty\n~~~~~~~~~~\n\nAs you might guess, you can add L1-norm for **L1 regularization**:\n\n\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2 + \\lambda\\sum_{j=0}^{d}\\left|w_j\\right|\\end{align}\n\nAgain, in fancy term, this loss function is also known as **Lasso\nregression**. Using matrix notation:\n\n\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2 + \\lambda{\\left\\lVert w \\right\\rVert}_1\\end{align}\n\nIt's more complex to get a closed-form solution for this, so we'll leave\nit here.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L1 vs L2 Penalty\n~~~~~~~~~~~~~~~~\n\nWhat's the point of using different penalty terms, as it seems like both\ntry to push down the size of $w$.\n\nTurns out L1 penalty tends to produce *sparse solutions*, which means\nmost entries in $w$ are zeros. This is great if you want the model\nto be simple and compact.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nnp.random.seed(1)\n\nLambda = 0.2\n\nTrue_w1 = 300\nTrue_w0 = 500\ndef true_target(x):\n  return True_w1*x + True_w0\n\ndef observed_target(x):\n  \"\"\"Underlying data with Gaussian noise added\"\"\"\n  normal_noise = np.random.normal() * 6\n  return true_target(x) + normal_noise\n\nN = 50\n\n# Features, X is [1,50]\n# X = np.arange(N).reshape(N, 1)\nX = np.random.rand(N).reshape(N, 1)\n\n# Observed targets\ny = np.array([observed_target(x) for x in X]).reshape(N, 1)\n\n# Append 1 for intercept term later\nX = np.hstack([np.ones((N, 1)), X])\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Ranges of w0 and w1 to see, centering at the true line\nspanning_radius = np.max([True_w1, True_w0, 200])\nstep = 0.5\nw0range = np.arange(True_w0-spanning_radius, True_w0+spanning_radius, step)\nw1range = np.arange(True_w1-spanning_radius, True_w1+spanning_radius, step)\nw0grid, w1grid = np.meshgrid(w0range, w1range)\n\nrange_len = len(w0range)\nprint(\"Number of values in each axis:\", range_len)\n\n# Make [w0, w1] in (2, 14400) shape\nall_w0w1_values = np.hstack([w0grid.flatten()[:,None], w1grid.flatten()[:,None]]).T\n\n# Compute raw losses\nraw_losses = np.linalg.norm(y - (X @ all_w0w1_values), axis=0) ** 2\nraw_losses = raw_losses.reshape((range_len, range_len))\n\n# Compute L2 penalty losses\npenalty_loss = Lambda * np.linalg.norm(all_w0w1_values, axis=0) ** 2\npenalty_loss = penalty_loss.reshape((range_len, range_len))\n\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\n\nlr = LinearRegression(fit_intercept=False)\nlr = lr.fit(X, y)\n\nridge = Ridge(alpha=Lambda)\nridge = ridge.fit(X, y)\n\nlasso = Lasso(alpha=Lambda)\nlasso = lasso.fit(X, y)\n\nridge_coef = ridge.coef_[0]\nlasso_coef = lasso.coef_\nlr_coef = lr.coef_[0]\n\nprint(\"Ridge solution:\", ridge_coef)\nprint(\"Lasso solution:\", lasso_coef)\nprint(\"Linear Regression solution:\", lr_coef)\n\nridge_loss = np.linalg.norm(y - X @ ridge_coef) ** 2 + Lambda * np.linalg.norm(ridge_coef, ord=2) ** 2\nlasso_loss = np.linalg.norm(y - X @ lasso_coef) ** 2 + Lambda * np.linalg.norm(lasso_coef, ord=1)\nlr_loss = np.linalg.norm(y - X @ lr_coef) ** 2\n\nprint(\"Ridge loss:\", ridge_loss)\nprint(\"Lasso loss:\", lasso_loss)\nprint(\"Linear Regression loss:\", lr_loss)\n\nfig = plt.figure(figsize=(10,6))\nax = fig.gca(projection='3d')\n\nax.plot_surface(w0grid, w1grid, raw_losses, alpha=0.4, cmap='RdBu')\nax.contour(w0grid, w1grid, penalty_loss, alpha=1, cmap='plasma_r')\n\nax.scatter([0],[0], 0, c='black', s=100, label=\"(0,0)\")\n\nax.scatter([ridge_coef[0]],[ridge_coef[1]], 0, c='green', s=100, label=\"Ridge solution\")\nax.scatter([lr_coef[0]],[lr_coef[1]], 0, c='red', s=100, label=\"Linear regression solution\")\n\nax.legend(loc='best')\nax.set_xlabel('w[0]')\nax.set_ylabel('w[1]')\nax.set_zlabel('L(w)')\nax.set_xticks(np.arange(True_w0-spanning_radius, True_w0+spanning_radius, 100))\nax.set_yticks(np.arange(True_w1-spanning_radius, True_w1+spanning_radius, 100))\nax.set_zticks([ridge_loss])\n# plt.axis('off')\n# ax.set_zlim(0, 20000)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}